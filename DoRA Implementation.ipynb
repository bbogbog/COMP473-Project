{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beae749c",
   "metadata": {},
   "source": [
    "# Implementing DoRA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f64673",
   "metadata": {},
   "source": [
    "### LoRA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f61fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e90aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithDoRAMerged(nn.Module):\n",
    "\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "        self.m = nn.Parameter(\n",
    "            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        numerator = self.linear.weight + self.lora.alpha*lora.T\n",
    "        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n",
    "        directional_component = numerator / denominator\n",
    "        new_weight = self.m * directional_component\n",
    "        return F.linear(x, new_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ded9d",
   "metadata": {},
   "source": [
    "#### Linear layer to replace that of model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350e06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40e894",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd846f0b",
   "metadata": {},
   "source": [
    "#### Standard Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60e96b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "layer = nn.Linear(10, 2)\n",
    "x = torch.randn((1, 10))\n",
    "\n",
    "print(\"Original output:\", layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4804f",
   "metadata": {},
   "source": [
    "#### LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8f9e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_lora_1 = LinearWithLoRA(layer, rank=2, alpha=4)\n",
    "print(\"LoRA output:\", layer_lora_1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611472f8",
   "metadata": {},
   "source": [
    "#### Perceptron model with 3 hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df1e7da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m MultilayerPerceptron(\n\u001b[1;32m---> 20\u001b[0m     num_features\u001b[38;5;241m=\u001b[39mnum_features,\n\u001b[0;32m     21\u001b[0m     num_hidden_1\u001b[38;5;241m=\u001b[39mnum_hidden_1,\n\u001b[0;32m     22\u001b[0m     num_hidden_2\u001b[38;5;241m=\u001b[39mnum_hidden_2, \n\u001b[0;32m     23\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_features' is not defined"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "        num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MultilayerPerceptron(\n",
    "    num_features=num_features,\n",
    "    num_hidden_1=num_hidden_1,\n",
    "    num_hidden_2=num_hidden_2, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991f6a9",
   "metadata": {},
   "source": [
    "### DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697be05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Freeze parameter weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a674af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the IMDB dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c202c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29486a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved train data to: data\\train.csv\n",
      "Successfully saved validation data to: data\\validation.csv\n",
      "Successfully saved test data to: data\\test.csv\n"
     ]
    }
   ],
   "source": [
    "# Split the existing 'train' split into a new 'train' (80%) and 'validation' (20%)\n",
    "train_test_split = imdb_dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "final_train_data = train_test_split[\"train\"]\n",
    "final_val_data = train_test_split[\"test\"]  \n",
    "final_test_data = imdb_dataset[\"test\"] \n",
    "\n",
    "# Define the directory to save the CSVs\n",
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to convert and save a split\n",
    "def save_split_to_csv(dataset_split, name):\n",
    "    # Convert Hugging Face Dataset to Pandas DataFrame\n",
    "    df = pd.DataFrame(dataset_split)\n",
    "    \n",
    "    # Define the file path\n",
    "    file_path = os.path.join(output_dir, f\"{name}.csv\")\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Successfully saved {name} data to: {file_path}\")\n",
    "\n",
    "# Save all three supervised splits\n",
    "save_split_to_csv(final_train_data, \"train\")\n",
    "save_split_to_csv(final_val_data, \"validation\")\n",
    "save_split_to_csv(final_test_data, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ecac2",
   "metadata": {},
   "source": [
    "#### Adapting Model to DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fff3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data\", \"validation.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3918ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "from local_dataset_utilities import tokenization, setup_dataloaders, get_dataset\n",
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.W_a = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.W_b = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.W_a @ self.W_b)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='LoRA parameters configuration')\n",
    "    parser.add_argument('--lora_r', type=int, default=8, help='Rank for LoRA layers')\n",
    "    parser.add_argument('--lora_alpha', type=int, default=16, help='Alpha for LoRA layers')\n",
    "    parser.add_argument('--lora_query', type=str2bool, default=True, help='Apply LoRA to query')\n",
    "    parser.add_argument('--lora_key', type=str2bool, default=False, help='Apply LoRA to key')\n",
    "    parser.add_argument('--lora_value', type=str2bool, default=True, help='Apply LoRA to value')\n",
    "    parser.add_argument('--lora_projection', type=str2bool, default=False, help='Apply LoRA to projection')\n",
    "    parser.add_argument('--lora_mlp', type=str2bool, default=False, help='Apply LoRA to MLP')\n",
    "    parser.add_argument('--lora_head', type=str2bool, default=False, help='Apply LoRA to head')\n",
    "    parser.add_argument('--device', type=int, default=0, help='Specify GPU device index')\n",
    "    parser.add_argument('--verbose', type=str2bool, default=True, help='Enable/disable progress bars')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Please switch to a GPU machine before running this code.\")\n",
    "        quit()\n",
    "\n",
    "    df_train, df_val, df_test = get_dataset()\n",
    "    imdb_tokenized = tokenization()\n",
    "    train_loader, val_loader, test_loader = setup_dataloaders(imdb_tokenized)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=2\n",
    "    )\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    assign_lora = partial(LinearWithLoRA, rank=args.lora_r, alpha=args.lora_alpha)\n",
    "\n",
    "    for layer in model.distilbert.transformer.layer:\n",
    "        if args.lora_query:\n",
    "            layer.attention.q_lin = assign_lora(layer.attention.q_lin)\n",
    "        if args.lora_key:\n",
    "            layer.attention.k_lin = assign_lora(layer.attention.k_lin)\n",
    "        if args.lora_value:\n",
    "            layer.attention.v_lin = assign_lora(layer.attention.v_lin)\n",
    "        if args.lora_projection:\n",
    "            layer.attention.out_lin = assign_lora(layer.attention.out_lin)\n",
    "        if args.lora_mlp:\n",
    "            layer.ffn.lin1 = assign_lora(layer.ffn.lin1)\n",
    "            layer.ffn.lin2 = assign_lora(layer.ffn.lin2)\n",
    "    if args.lora_head:\n",
    "        model.pre_classifier = assign_lora(model.pre_classifier)\n",
    "        model.classifier = assign_lora(model.classifier)\n",
    "\n",
    "    print(\"Total number of trainable parameters:\", count_parameters(model))\n",
    "\n",
    "    lightning_model = CustomLightningModule(model)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "        )  # save top 1 model\n",
    "    ]\n",
    "    logger = CSVLogger(save_dir=\"logs/\", name=f\"my-model-{args.device}\")\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=3,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"16-mixed\",\n",
    "        devices=[int(args.device)],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "        enable_progress_bar=args.verbose\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    trainer.fit(\n",
    "        model=lightning_model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"Time elapsed {elapsed/60:.2f} min\")\n",
    "\n",
    "    train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\", verbose=False)\n",
    "    val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\", verbose=False)\n",
    "    test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)\n",
    "\n",
    "    # Print all argparse settings\n",
    "    print(\"------------------------------------------------\")\n",
    "    for arg in vars(args):\n",
    "        print(f'{arg}: {getattr(args, arg)}')\n",
    "\n",
    "    train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\", verbose=False)\n",
    "    val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\", verbose=False)\n",
    "    test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)\n",
    "\n",
    "    # Print settings and results\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        s = \"------------------------------------------------\"\n",
    "        print(s), f.write(s+\"\\n\")        \n",
    "        for arg in vars(args):\n",
    "            s = f'{arg}: {getattr(args, arg)}'\n",
    "            print(s), f.write(s+\"\\n\")\n",
    "\n",
    "        s = f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\"\n",
    "        print(s), f.write(s+\"\\n\")\n",
    "        s = f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\"\n",
    "        print(s), f.write(s+\"\\n\")\n",
    "        s = f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\"\n",
    "        print(s), f.write(s+\"\\n\")\n",
    "        s = \"------------------------------------------------\"\n",
    "        print(s), f.write(s+\"\\n\")    \n",
    "\n",
    "    # Cleanup\n",
    "    log_dir = f\"logs/my-model-{args.device}\"\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
